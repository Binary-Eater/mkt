From 8865689f70c10a1be72a86cf71a503767c7a8121 Mon Sep 17 00:00:00 2001
From: Hasan Emar <hasane@mellanox.com>
Date: Mon, 3 Aug 2020 21:06:42 +0300
Subject: [PATCH 2/3] mlx5+infra: Added support to RDMA READ with ODP

Some issues in duplicate RDMA flow fixed.

Signed-off-by: Hasan Emar <hasane@mellanox.com>
Signed-off-by: Leon Romanovsky <leonro@nvidia.com>
---
 mellanox/mlx5/mlx5_config_defaults.c  |  2 +-
 mellanox/mlx5/mlx5_dataif_common.c    |  4 +-
 mellanox/mlx5/mlx5_dataif_transport.c | 57 ++++++++++++++++++++++++++-
 mellanox/mlx5/mlx5_dataif_utils.c     |  1 +
 mellanox/mlx5/mlx5_dataif_utils.h     |  1 +
 mellanox/mlx5/mlx5_dataif_wqe.c       | 33 +++++++++++++---
 mlnx_infra/simx-qemu.cfg              |  2 +-
 7 files changed, 89 insertions(+), 11 deletions(-)

diff --git a/mellanox/mlx5/mlx5_config_defaults.c b/mellanox/mlx5/mlx5_config_defaults.c
index 022b6c97a5a8..bb126ed72671 100644
--- a/mellanox/mlx5/mlx5_config_defaults.c
+++ b/mellanox/mlx5/mlx5_config_defaults.c
@@ -391,7 +391,7 @@ static const MLX5Configuration mlx5_config_defaults = {
         .send = true,
         .receive = true,
         .write = true,
-        .read = false,
+        .read = true,
         .atomic = false,
         .rmp = true,
     },
diff --git a/mellanox/mlx5/mlx5_dataif_common.c b/mellanox/mlx5/mlx5_dataif_common.c
index 2c95625e1b50..237375487a68 100644
--- a/mellanox/mlx5/mlx5_dataif_common.c
+++ b/mellanox/mlx5/mlx5_dataif_common.c
@@ -1311,9 +1311,11 @@ int mlx5_get_index_of_duplicate_atomic_rdma_read_context(const MLX5State *s, uin
             }
         } else {
             uint32_t num_of_rdma_read_response_packets = mlx5_get_num_of_expected_rdma_read_response_packets(s, qp_num, cur_wq->recv_wq_context.rdma_dma_length);
+            uint32_t last_psn = cur_wq->recv_wq_context.duplicate_atomic_rdma_read_context_arr[i].requestor_psn +
+                                cur_wq->recv_wq_context.duplicate_atomic_rdma_read_context_arr[i].rdma_read_context.original_number_of_dma_responsed_packets;

             if (psn >= cur_wq->recv_wq_context.duplicate_atomic_rdma_read_context_arr[i].requestor_psn &&
-                psn < cur_wq->recv_wq_context.duplicate_atomic_rdma_read_context_arr[i].requestor_psn + num_of_rdma_read_response_packets) {
+                (psn + num_of_rdma_read_response_packets <= last_psn)) {

                 if (!mlx5_validate_psn_and_address_rdma_read_request(s, qp_num, remote_addr, cur_wq->recv_wq_context.duplicate_atomic_rdma_read_context_arr[i].address,
                                                                      cur_wq->recv_wq_context.rdma_dma_length, psn,
diff --git a/mellanox/mlx5/mlx5_dataif_transport.c b/mellanox/mlx5/mlx5_dataif_transport.c
index 87f0fc16f439..0827a87d0e44 100644
--- a/mellanox/mlx5/mlx5_dataif_transport.c
+++ b/mellanox/mlx5/mlx5_dataif_transport.c
@@ -1587,6 +1587,11 @@ static int mlx5_handle_rdma_read_atomic_responsed(MLX5State *s, uint32_t qp_num,
     } else {
         ret = mlx5_write_data_to_buffer(s, &ds_ctx, payload_ptr, payload_size, qp_num, mlx5_common_wq_get_pd(s, qp_num, UINT32_MAX, true));
         if (ret) {
+            if (cur_wq->odp.page_fault) {
+                MLX_LOG_DBG(WQ, "QP 0x%x: Page fault while trying to write RDMA READ response\n", qp_num);
+                mlx5_eq_generate_wqe_associated_page_fault_event(s, 0, send_ctx->wqe_index, payload_size,
+                                                                 MLX5_PAGE_FAULT_TYPE_REQUESTER_QP_WQE_ON_READ_OR_ATOMIC, qp_num, MLX5_QP_REQUESTER_WRITE_PAGE_FAULT, UINT32_MAX);
+            }
             MLX_LOG_ERR(WQ, "QP 0x%x: Failed to write data to WQE: ret = %d\n", qp_num, ret);
             return ret;
         }
@@ -2092,12 +2097,24 @@ static void mlx5_handle_ack_responed(MLX5State *s, uint32_t dest_qp, MLX5Receive
                                      uint8_t packet_opcode, uint32_t current_psn)
 {
     MLX5SendContext *send_ctx = NULL;
+
+    MLX5CommonWQ *dest_wq = MLX5_GET_POINTER_TO_WQ_ELEMENT(s, dest_qp);
+    const MLX5SendCtxQueue *send_ctx_queue = dest_wq->send_wq_context.send_ctx_queue;
+    MLX5SendContext *tmp_send_ctx;
+    size_t num_of_send_ctx_entries = mlx5_send_ctx_queue_get_used_entries_number(send_ctx_queue);
+    int index = 0;
+
     int ret;
     uint8_t syndrome = 0;
     uint8_t nak_type = 0;

     MLX_ASSERT_CMPINT(mlx5_is_reliable_transport_type(source_qp_type), ==, true);

+    if ((dest_wq->odp.cur_page_faults_types & MLX5_QP_REQUESTER_WRITE_PAGE_FAULT) || dest_wq->odp.need_to_retransmit) {
+        MLX_LOG_DBG(WQ, "QP (0x%x) can't receive packet, page fault type (0x%x) not fixed yet\n", dest_qp, MLX5_QP_REQUESTER_WRITE_PAGE_FAULT);
+        return;
+    }
+
     if (receive_packet->aeth_header) {
         syndrome = MLX5_GET(aeth, receive_packet->aeth_header, syndrome);
         nak_type = (syndrome & MLX5_ACK_SYNDROME_TYPE_MASK) >> 5;
@@ -2117,7 +2134,24 @@ static void mlx5_handle_ack_responed(MLX5State *s, uint32_t dest_qp, MLX5Receive

         mlx_mutex_unlock(&s->qp_gen_cqe_lock);
         return;
-    } else if ((receive_packet->aeth_header) && (nak_type == MLX5_IB_PACKET_AETH_SYNDROME_ACK)) {
+    }
+
+    tmp_send_ctx = &send_ctx_queue->arr[send_ctx_queue->head % mlx5_send_ctx_queue_get_size(send_ctx_queue)];
+    while ((tmp_send_ctx != send_ctx) && (index < num_of_send_ctx_entries)) {
+        if (!(tmp_send_ctx->is_last_psn_sent_acked)) {
+            if (wqe_opcode_att[tmp_send_ctx->wqe_opcode] & (MLX5_OPER_RDMA_READ | MLX5_OPER_ATOMIC)) {
+                if (tmp_send_ctx->page_fault_response_retransmition) {
+                    MLX_LOG_DBG(WQ, "Dest QP 0x%x: Can't accept packet with psn (0x%x), a request before is didn't got all of the response\n",
+                                dest_qp, current_psn);
+                    return;
+                }
+            }
+        }
+        ++index;
+        tmp_send_ctx = &send_ctx_queue->arr[(send_ctx_queue->head + index) % mlx5_send_ctx_queue_get_size(send_ctx_queue)];
+    }
+
+    if ((receive_packet->aeth_header) && (nak_type == MLX5_IB_PACKET_AETH_SYNDROME_ACK)) {
         /* Handling coalesced ACK (if there any) */
         ret = mlx5_handle_coalesced_ack(s, dest_qp, send_ctx);
         if (ret) {
@@ -2195,9 +2229,26 @@ static void mlx5_handle_ack_responed(MLX5State *s, uint32_t dest_qp, MLX5Receive
             /* If we got an error while handling RDMA Read/Atomic, No need to keep handling the following responses */
             ret = mlx5_handle_rdma_read_atomic_responsed(s, dest_qp, receive_packet->payload_ptr, receive_packet->payload_size, send_ctx);
             if (ret) {
+                if (s->common_wq_arr[dest_qp].odp.page_fault) {
+                    send_ctx->expected_rdma_read_response_psn = (send_ctx->expected_rdma_read_response_psn - 1) & MLX5_MAX_IB_PSN_VALUE;
+                    send_ctx->psn_of_last_packet_that_was_acked = (send_ctx->expected_rdma_read_response_psn - 1) & MLX5_MAX_IB_PSN_VALUE;
+
+                    send_ctx->psn_of_retransmition_after_page_fault = send_ctx->expected_rdma_read_response_psn;
+                    send_ctx->page_fault_response_retransmition = true;
+                    send_ctx->is_last_psn_sent_acked = false;
+
+                    dest_wq->odp.page_fault = false;
+                    dest_wq->odp.need_to_retransmit = true;
+
+                    mlx_mutex_unlock(&s->qp_gen_cqe_lock);
+                    return;
+                }
                 send_ctx->syndrome = ret;
             } else {
                 send_ctx->psn_of_last_packet_that_was_acked = current_psn;
+                if (send_ctx->is_last_psn_sent_acked) {
+                    send_ctx->page_fault_response_retransmition = false;
+                }
             }
         }
     }
@@ -2859,7 +2910,9 @@ static void mlx5_handle_send_ctx_for_ib_roce_packet(MLX5State *s, uint32_t sourc

         /* For RDMA_READ, the PSN of the last packet to ACK is depends on the number of response packets */
         if (ib_packet_opcode_att[packet_opcode] & MLX5_IB_RDMA_READ_REQUEST_OPCODE) {
-            send_ctx->last_psn_sent = (send_ctx->first_psn_sent + (send_ctx->number_of_rdma_response_packets - 1)) & MLX5_MAX_IB_PSN_VALUE;
+            if (send_ctx->psn_of_last_packet_that_was_acked == UINT32_MAX) {
+                send_ctx->last_psn_sent = (send_ctx->first_psn_sent + (send_ctx->number_of_rdma_response_packets - 1)) & MLX5_MAX_IB_PSN_VALUE;
+            }
             if (cur_wq->qp.st == MLX5_QPC_ST_DCI) {
                 /* Need to add one more packet for the connect packet */
                 send_ctx->last_psn_sent = (send_ctx->last_psn_sent + 1) & MLX5_MAX_IB_PSN_VALUE;
diff --git a/mellanox/mlx5/mlx5_dataif_utils.c b/mellanox/mlx5/mlx5_dataif_utils.c
index 1e6680a3ed97..874f53aeda1a 100644
--- a/mellanox/mlx5/mlx5_dataif_utils.c
+++ b/mellanox/mlx5/mlx5_dataif_utils.c
@@ -50,6 +50,7 @@ static const MLX5SendContext default_send_ctx = {
     .is_multicast                           = false,
     .is_rnr_nak                             = false,
     .is_sequence_nak                        = false,
+    .page_fault_response_retransmition      = false,
 };

 MLX5SendCtxQueue *mlx5_send_ctx_queue_new(unsigned long size, const char *name)
diff --git a/mellanox/mlx5/mlx5_dataif_utils.h b/mellanox/mlx5/mlx5_dataif_utils.h
index e20f5bc597a1..f4be91a0c0bb 100644
--- a/mellanox/mlx5/mlx5_dataif_utils.h
+++ b/mellanox/mlx5/mlx5_dataif_utils.h
@@ -141,6 +141,7 @@ typedef struct MLX5SendContext {
     bool is_multicast;
     bool is_rnr_nak;
     bool is_sequence_nak;
+    bool page_fault_response_retransmition;
 } MLX5SendContext;

 typedef struct MLX5SendCtxQueue {
diff --git a/mellanox/mlx5/mlx5_dataif_wqe.c b/mellanox/mlx5/mlx5_dataif_wqe.c
index f820d4e476c4..f90cf91a2790 100644
--- a/mellanox/mlx5/mlx5_dataif_wqe.c
+++ b/mellanox/mlx5/mlx5_dataif_wqe.c
@@ -7038,17 +7038,21 @@ static int mlx5_create_packet_payload(MLX5State *s, MLX5DsCtx *ds_ctx, void *pay
                                      MLX5_MEMORY_ACCESS_PERMISSION_LOCAL_READ);
                 if (ret) {
                     if (s->common_wq_arr[qp_num].odp.page_fault) {
+                        uint8_t page_fault_type = 0;
+
                         MLX_LOG_DBG(WQ, "Send QP 0x%x: Page fault while trying to read data from DS[%d] that points to M_KEY 0x%x with address 0x%"PRIx64"\n",
                                     qp_num, ds_ctx->ds_idx, m_key_idx, cur_addr + ds_ctx->cur_ds_offset);

                         if (wqe_opcode_att[opcode] & (MLX5_OPER_SEND | MLX5_OPER_RDMA_WRITE)) {
-                            mlx5_eq_generate_wqe_associated_page_fault_event(s, *payload_copied_data, mlx5_common_wq_get_sq_hw_counter(s, qp_num), payload_size,
-                                                                             MLX5_PAGE_FAULT_TYPE_REQUESTER_QP_WQE_ON_SEND_OR_WRITE,
-                                                                             qp_num, MLX5_QP_REQUESTER_READ_PAGE_FAULT, UINT32_MAX);
+                            page_fault_type = MLX5_PAGE_FAULT_TYPE_REQUESTER_QP_WQE_ON_SEND_OR_WRITE;
+                        } else if (wqe_opcode_att[opcode] & (MLX5_OPER_RDMA_READ)) {
+                            page_fault_type = MLX5_PAGE_FAULT_TYPE_REQUESTER_QP_WQE_ON_READ_OR_ATOMIC;
                         } else {
                             MLX_LOG_ERR(WQ, "ODP is not supported yet for operations with opcode 0x%x\n", opcode);
                             goto out;
                         }
+                        mlx5_eq_generate_wqe_associated_page_fault_event(s, *payload_copied_data, mlx5_common_wq_get_sq_hw_counter(s, qp_num), payload_size,
+                                                                         page_fault_type, qp_num, MLX5_QP_REQUESTER_READ_PAGE_FAULT, UINT32_MAX);
                     } else {
                         MLX_LOG_ERR(WQ, "Send QP 0x%x: Failed to read data of DS[%d] that points to M_KEY 0x%x with address 0x%"PRIx64": ret = %d\n",
                                     qp_num, ds_ctx->ds_idx, m_key_idx, cur_addr + ds_ctx->cur_ds_offset, ret);
@@ -8847,6 +8851,21 @@ static int mlx5_handle_rdma_atomic_for_receiver(MLX5State *s, uint32_t qp_num, c
                 ret = mlx5_read_data(s, remote_key, remote_addr + dest_wq->recv_wq_context.msg_offset, read_responed_payload_size, read_responed_payload_ptr, qp_num, false,
                                      MLX5_MEMORY_ACCESS_PERMISSION_REMOTE_READ);
                 if (ret) {
+                    if (dest_wq->odp.page_fault) {
+                        MLX_LOG_DBG(WQ, "QP 0x%x: Page fault occurred while trying to handle incoming RDMA READ\n", qp_num);
+
+                        mlx5_eq_generate_rdma_page_fault_event(s, 0, remote_key, dest_wq->recv_wq_context.rdma_dma_length, dest_wq->recv_wq_context.rdma_dma_length, remote_addr,
+                                                               MLX5_PAGE_FAULT_TYPE_RDMA_READ, MLX5_QP_RESPONDER_READ_PAGE_FAULT, qp_num);
+
+                        if (!is_duplicate_rdma_read_request) {
+                            mlx_mutex_lock(&dest_wq->recv_wq_context.duplicate_atomic_rdma_read_lock);
+                            mlx5_add_new_entry_to_duplicate_atomic_rdma_read_context_buf(s, qp_num, MLX5_GET(bth, receive_packet->bth_header, psn),
+                                                                                         false, UINT64_MAX, dest_wq->recv_wq_context.rdma_dma_length, remote_addr, remote_key);
+                            mlx_mutex_unlock(&dest_wq->recv_wq_context.duplicate_atomic_rdma_read_lock);
+                        }
+                        goto handle_q_counter;
+                    }
+
                     MLX_LOG_ERR(WQ, "Responder QP 0x%x: Failed to read data of M_KEY 0x%x with address 0x%"PRIx64": ret = %d\n", qp_num, remote_key, remote_addr, ret);

                     if (!is_duplicate_rdma_read_request) {
@@ -8878,7 +8897,8 @@ static int mlx5_handle_rdma_atomic_for_receiver(MLX5State *s, uint32_t qp_num, c
                 }
             }

-            if (!is_duplicate_rdma_read_request) {
+            if (!is_duplicate_rdma_read_request &&
+                (mlx5_get_index_of_duplicate_atomic_rdma_read_context(s, qp_num, MLX5_GET(bth, receive_packet->bth_header, psn), remote_addr, remote_key, false) < 0)) {
                 mlx_mutex_lock(&dest_wq->recv_wq_context.duplicate_atomic_rdma_read_lock);
                 mlx5_add_new_entry_to_duplicate_atomic_rdma_read_context_buf(s, qp_num, MLX5_GET(bth, receive_packet->bth_header, psn),
                                                                              false, UINT64_MAX, dest_wq->recv_wq_context.rdma_dma_length, remote_addr, remote_key);
@@ -9370,7 +9390,7 @@ int mlx5_handle_receive_packet(MLX5State *s, MLX5ReceivePacket *receive_packet,

     /* handle RDMA opcodes */
     if (wqe_opcode_att[opcode] & (MLX5_OPER_RDMA_WRITE | MLX5_OPER_RDMA_READ | MLX5_OPER_ATOMIC)) {
-        if (dest_wq->odp.cur_page_faults_types & MLX5_QP_RESPONDER_WRITE_PAGE_FAULT) {
+        if (dest_wq->odp.cur_page_faults_types & (MLX5_QP_RESPONDER_WRITE_PAGE_FAULT | MLX5_QP_RESPONDER_READ_PAGE_FAULT)) {
             MLX_LOG_DBG(WQ, "DEST QP 0x%x: already triggered page fault event and not fixed yet\n", destination_qp_num);

             ret = MLX5_COMPLETION_WITH_ERROR_CQE_SYNDROME_LOCAL_ACCESS_ERROR;
@@ -13902,7 +13922,8 @@ static void mlx5_send_retransmit_ib_packet(MLX5State *s, uint32_t qp_num, MLX5Se
     }

     if ((MLX5_IS_PAGE_FAULT_PSN(send_ctx->psn_of_retransmition_after_page_fault)) &&
-        (!send_ctx->need_to_retransmit_packet)) {
+        (!send_ctx->need_to_retransmit_packet) &&
+        (!send_ctx->page_fault_response_retransmition)) {
         if ((send_ctx->first_psn_sent == UINT32_MAX) ||
             (send_ctx->psn_of_last_packet_that_was_acked == send_ctx->last_psn_sent)) {
             /* No packets sent in this ctx (page fault occurred while creating the first packet payload)
diff --git a/mlnx_infra/simx-qemu.cfg b/mlnx_infra/simx-qemu.cfg
index 998176a5cbc5..939bd6524c2d 100644
--- a/mlnx_infra/simx-qemu.cfg
+++ b/mlnx_infra/simx-qemu.cfg
@@ -5046,7 +5046,7 @@
 #write = true

 # If set, RDMA read operations are supported
-#read = false
+#read = true

 # If set, atomic operatiions are supported
 #atomic = false
--
2.26.2

